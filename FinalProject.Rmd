---
title: "Analysis of User Behavior in Tourism Portal"
output:  
  html_document:
    theme: united
    highlight: tango
    toc: TRUE
    toc_depth: 4
    toc_float:
      smooth_scroll: TRUE
      collapsed: FALSE
---

## Gonzalo Valdenebro

[LinkedIn](https://www.linkedin.com/in/gonzalo-valdenebro-035392157/) \| [GitHub](https://github.com/gonzalovaldenebro)

# 1. Introduction to data

This [dataset](https://www.kaggle.com/datasets/ddosad/customer-behaviour-tourism-portal) provides a snapshot of user behavior on a social media platform associated with a tourism company. It includes essential information such as user identification, ticket purchase predictions, and key metrics related to user engagement, preferences, and demographic details. The [dataset](https://www.kaggle.com/datasets/ddosad/customer-behaviour-tourism-portal) aims to unveil insights that can inform strategic decisions to enhance the customer experience and increase ticket sales for the tourism company.

# 2. Research Questions of the client

### Research Question 1: Impact of Social Engagement

**Client's Inquiry:** We were curious about the impact of social engagement, specifically regarding check-in likes. How does a customer's involvement in giving likes during other check-ins influence their likelihood of purchasing a ticket next month?

### Research Question 2: Effect of Company Engagement

**Client's Inquiry:** The client is interested in understanding the influence of customers following their company's account on their likelihood of purchasing a ticket. How does being a CompanyFollower affect the chances of buying a ticket next month?

### Research Question 3: Website Interaction and Ticket Purchases

**Client's Inquiry:** The client is interested in exploring the relationship between the number of visits to our website and the likelihood of customers purchasing a ticket. How do YearlyAvgPageVisits correlate with the odds of buying a ticket next month?

# 3. Cleaning the environment and loading packages

```{r}
rm(list = ls())
```

```{r}
library(rpart)
library(rpart.plot)
library(ggplot2)
library(pROC)
library(explore)
library(RColorBrewer)
library(tidyverse)
library(dplyr)
library(kableExtra)
library(viridis)
library(RColorBrewer)
library(bookdown)
```

# 4. Loading the data

```{r}
cv = read.csv('Data/CustomerBehaviourTourism.csv', stringsAsFactors = TRUE) 
```

This is my data. It is cool. There are `r nrow(cv)` rows in my data.

# 5. Exploratory Data Analysis

## 5.1. Exploring the data structure

```{r}
#str(cv)
```

## 5.2. Exploring the summary statistics

```{r}
#summary(cv)
```

## 5.3. Missing Values

-   **Yearly_avg_view_on_travel_page** *[int]*: `r sum(is.na(cv$Yearly_avg_view_on_travel_page))`
-   **total_likes_on_outstation_checkin_given** *[int]*: `r sum(is.na(cv$total_likes_on_outstation_checkin_given))`
-   **Yearly_avg_comment_on_travel_page** *[int]*: `r sum(is.na(cv$Yearly_avg_comment_on_travel_page))`
-   **Adult_flag** *[int]*: `r sum(is.na(cv$Adult_flag))`
-   **Daily_Avg_mins_spend_on_traveling_page** *[int]*: `r sum(is.na(cv$Daily_Avg_mins_spend_on_traveling_page))`

# 6. Data Cleaning

### 6.2.1. Clean the **following_company_page** column

The **following_company_page** column should be either *"Yes"* or *"No"* values, but I come to find that it contains *`r unique(cv$following_company_page)`*. Which are distributed by the following order:

-   **0 :** `r sum(cv$following_company_page == 0)`
-   **1 :** `r sum(cv$following_company_page == 1)`
-   **Yes :** `r sum(cv$following_company_page == 'Yes')`
-   **No :** `r sum(cv$following_company_page == 'No')`
-   **Empty:** `r sum(cv$following_company_page == "")`
-   **Yeso :** `r sum(cv$following_company_page == 'Yeso')`

**Action Plan:** Collapse the **following_company_page** values to *"Yes"* and *"No"*, by making the *"1"* equal to *"Yes"* and the *"0"* to *"No"*, since there is only `r sum(cv$following_company_page == 'Yeso')` observation with the value *"Yeso"*, I will delete that row.

```{r}
cv <- cv %>%
  mutate(CompanyFollower = ifelse(following_company_page %in% c(0, "No"), "No",
                                  ifelse(following_company_page %in% c(1, "Yes"), "Yes", NA)))
```

```{r}
# Function to impute mode
impute_mode <- function(x) {
  mode_value <- names(sort(table(x), decreasing = TRUE))[1]
  x[x == "NA" | is.na(x)] <- mode_value
  return(x)
}

# Impute mode for the 'PreferredLocation' column
cv$CompanyFollower <- impute_mode(cv$CompanyFollower)
```

```{r}
cv %>%
  group_by(CompanyFollower, Taken_product) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = reorder(factor(CompanyFollower), count), y = count, fill = Taken_product)) +
  geom_bar(stat = "identity", position = "fill") +
  coord_flip() +
  labs(title = "Customer Following Page  & Buying a Ticket",
       x = "Customer Follow's Page",
       y = "Proportion",
       fill = "Buys a ticket next month") +
  scale_fill_brewer(palette = "Paired") +  # Use "Paired" palette
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### 6.2.2. Clean the **member_in_family** column

The **member_in_family** column should have only *1, 2, 3, 4, 5 and 10*, but I found that it contains the following *`r unique(cv$member_in_family)`*. Which are distributed by the following order:

-   **1 :** `r sum(cv$member_in_family == 1)`
-   **2 :** `r sum(cv$member_in_family == 2)`
-   **3 :** `r sum(cv$member_in_family == 3)`
-   **4 :** `r sum(cv$member_in_family == 4)`
-   **5 :** `r sum(cv$member_in_family == 5)`
-   **10 :** `r sum(cv$member_in_family == 10)`
-   **Three:** `r sum(cv$member_in_family == "Three")`

**Action Plan:** Collapse the **member_in_family** values to *1, 2, 3, 4, 5, 10* and the observations containing the *"Three"* values, equal to *"3"* and given that we only have `r sum(cv$member_in_family == 5)` for family size of 5 and `r sum(cv$member_in_family == 10)` for family size of 10, I am going to collapse them into a value called "5+" so that it does not affect the model prediction

```{r}
cv <- cv %>%
  mutate(FamilyMembers = case_when(
    member_in_family == "Three" ~ "3",
    member_in_family %in% c("5", "10") ~ "5+",
    TRUE ~ as.character(member_in_family)
  ))
```

#### 6.2.2.1. Graphing the **member_in_family** column

```{r}
cv %>%
  group_by(FamilyMembers, Taken_product) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = reorder(factor(FamilyMembers), count), y = count, fill = Taken_product)) +
  geom_bar(stat = "identity", position = "fill") +
  coord_flip() +
  labs(title = "Customer Family Member's & Buying a Ticket",
       x = "Customer Family Member's",
       y = "Proportion",
       fill = "Buys a ticket next month") +
  scale_fill_brewer(palette = "Paired") +  # Use "Paired" palette
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### 6.2.3. Clean the **working_flag** column

The **working_flag** column should have only *"Yes"* or *"No"* values, but I found that it contains the following *`r unique(cv$working_flag)`*. Which are distributed by the following order:

-   **Yes :** `r sum(cv$working_flag == "Yes")`
-   **No :** `r sum(cv$working_flag == "No")`
-   **0 :** `r sum(cv$working_flag == "0")`

**Action Plan:** Collapse the **working_flag** value of 0 to "Yes"

```{r}
cv$IsWorking <- ifelse(cv$working_flag == "0", "No", as.character(cv$working_flag))
```

#### 6.2.3.1. Graphing the **working_flag** column

```{r}
cv %>%
  group_by(IsWorking, Taken_product) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = reorder(factor(IsWorking), count), y = count, fill = Taken_product)) +
  geom_bar(stat = "identity", position = "fill") +
  coord_flip() +
  labs(title = "Customer Working Status & Buying a Ticket",
       x = "Customer Working Status",
       y = "Proportion",
       fill = "Buys a ticket next month") +
  scale_fill_brewer(palette = "Paired") +  # Use "Paired" palette
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### 6.2.4. Clean the **preferred_device** column

The **preferred_device** column should have only *"iOS"*, *"Android"*, *"iOS and Android"*, *"Tab"*, *"Laptop"*, *"Mobile"* values, but I found that it contains the following *`r unique(cv$preferred_device)`*. Which are distributed by the following order:

-   **iOS and Android :** `r sum(cv$preferred_device == "iOS and Android")`
-   **iOS :** `r sum(cv$preferred_device == "iOS")`
-   **ANDROID :** `r sum(cv$preferred_device == "ANDROID")`
-   **Android :** `r sum(cv$preferred_device == "Android")`
-   **Android OS :** `r sum(cv$preferred_device == "Android OS")`
-   **Other :** `r sum(cv$preferred_device == "Other")`
-   **Others :** `r sum(cv$preferred_device == "Others")`
-   **Tab :** `r sum(cv$preferred_device == "Tab")`
-   **Laptop :** `r sum(cv$preferred_device == "Laptop")`
-   **Mobile :** `r sum(cv$preferred_device == "Mobile")`
-   **Empty :** `r sum(cv$preferred_device == "")`

**Action Plan:** Collapse the **preferred_device** values to *"Mobile", "Laptop", "Tablet" and "Other"* and the impute the missing values with the mode of the column after collapsing them

```{r}
# Function to collapse device categories
collapse_device_categories <- function(device) {
  device <- tolower(device)
  if (device %in% c("android", "android os","ios","ios and android", "Mobile")) {
    return("Mobile")
  } else if (device %in% c("laptop")) {
    return("Laptop")
  } else if (device == "tab") {
    return("Tablet")
  } else if (device == "") {
    return("NA")
  } else{
    return("Other")
  }
}

# Apply the function to create the new column
cv$PreferedDevice <- sapply(cv$preferred_device, collapse_device_categories)
```

Impute the missing values with the mode

```{r}
# Function to impute mode
impute_mode <- function(x) {
  mode_value <- names(sort(table(x), decreasing = TRUE))[1]
  x[x == "NA" | is.na(x)] <- mode_value
  return(x)
}

# Impute mode for the 'PreferredLocation' column
cv$PreferedDevice <- impute_mode(cv$PreferedDevice)
```

#### 6.3.4.1. Graphing the **PreferedDevice** column

```{r}
cv %>%
  group_by(PreferedDevice, Taken_product) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = reorder(factor(PreferedDevice), count), y = count, fill = Taken_product)) +
  geom_bar(stat = "identity", position = "fill") +
  coord_flip() +
  labs(title = "Customer Prefered Device & Buying a Ticket",
       x = "Customer Prefered Device",
       y = "Proportion",
       fill = "Buys a ticket next month") +
  scale_fill_brewer(palette = "Paired") +  # Use "Paired" palette
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### 6.2.5. Clean the **preferred_location_type** column

The **preferred_location_type** column should have only "Beach", "Financial", "Historical site", "Medical", "Other", "Big Cities" and "(Other)" values, but I found that it contains the following *`r unique(cv$preferred_location_type)`*. Which are distributed by the following order:

-   **Beach :** `r sum(cv$preferred_location_type == "Beach")`
-   **Financial :** `r sum(cv$preferred_location_type == "Financial")`
-   **Historical site :** `r sum(cv$preferred_location_type == "Historical site")`
-   **Medical :** `r sum(cv$preferred_location_type == "Medical")`
-   **Big Cities :** `r sum(cv$preferred_location_type == "Big Cities")`
-   **Other :** `r sum(cv$preferred_location_type == "Other")`
-   **Game :** `r sum(cv$preferred_location_type == "Game")`
-   **Social media :** `r sum(cv$preferred_location_type == "Social media")`
-   **Entertainment :** `r sum(cv$preferred_location_type == "Entertainment")`
-   **Tour and Travel :** `r sum(cv$preferred_location_type == "Tour and Travel")`
-   **Tour Travel :** `r sum(cv$preferred_location_type == "Tour Travel")`
-   **Movie :** `r sum(cv$preferred_location_type == "Movie")`
-   **OTT(Over-the-top media services) :** `r sum(cv$preferred_location_type == "OTT")`
-   **Trekking :** `r sum(cv$preferred_location_type == "Trekking")`
-   **Hill Stations :** `r sum(cv$preferred_location_type == "Hill Stations")`
-   **Empty :** `r sum(cv$preferred_location_type == "")`

**Action Plan:** Collapse the **preferred_location_type** values to *"Metropolitan Focus", "Natural Exploration", "Cultural Exploration", "Media and Entertainment"* and the observations from the other categories to be merged into the *"Other"* category given that their frequency is not very significant and it might affect my model's results, the missing values will be imputed with the mode of the column after I collapse them

```{r}

collapse_categories <- function(category) {
  if (category %in% c("Financial", "Big Cities", "Medical")) {
    return("Metropolitan Focus")
  } else if (category %in% c("Beach", "Trekking", "Hill Stations")) {
    return("Natural Exploration")
  } else if (category %in% c("Tour and Travel", "Historical site")) {
    return("Cultural Exploration")
  } else if (category %in% c("Game", "Social Media", "Entertainment", "Movie", "OTT")) {
    return("Media and Entertainment")
  } else if (category == "") {
    return("NA")
  } else {
    return("Other")
  }
}

# Apply the function to create the new column
cv$PreferredLocation <- sapply(cv$preferred_location_type, collapse_categories)
```

Impute the mode in the missing values

```{r}
# Function to impute mode
impute_mode <- function(x) {
  mode_value <- names(sort(table(x), decreasing = TRUE))[1]
  x[x == "NA" | is.na(x)] <- mode_value
  return(x)
}

# Impute mode for the 'PreferredLocation' column
cv$PreferredLocation <- impute_mode(cv$PreferredLocation)
```

#### 6.2.5.1. Graphing the **PreferredLocation** column

```{r}
cv %>%
  group_by(PreferredLocation, Taken_product) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = reorder(factor(PreferredLocation), count), y = count, fill = Taken_product)) +
  geom_bar(stat = "identity", position = "fill") +
  coord_flip() +
  labs(title = "Customer Prefered Location & Buying a Ticket",
       x = "Customer Prefered Location",
       y = "Proportion",
       fill = "Buys a ticket next month") +
  scale_fill_brewer(palette = "Paired") +  # Use "Paired" palette
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### 6.2.6. Clean the **yearly_avg_Outstation_checkins** column

The **yearly_avg_Outstation_checkins** `r str(cv$yearly_avg_Outstation_checkins)` column should have only *1, 2, 3, 7, 9 and 10* values, but I found that it contains the following *`r unique(cv$yearly_avg_Outstation_checkins)`*. Which are distributed by the following order:

-   **1 :** `r sum(cv$yearly_avg_Outstation_checkins == "1")`
-   **2 :** `r sum(cv$yearly_avg_Outstation_checkins == "2")`
-   **3 :** `r sum(cv$yearly_avg_Outstation_checkins == "3")`
-   **4 :** `r sum(cv$yearly_avg_Outstation_checkins == "4")`
-   **5 :** `r sum(cv$yearly_avg_Outstation_checkins == "5")`
-   **6 :** `r sum(cv$yearly_avg_Outstation_checkins == "6")`
-   **7 :** `r sum(cv$yearly_avg_Outstation_checkins == "7")`
-   **8 :** `r sum(cv$yearly_avg_Outstation_checkins == "8")`
-   **9 :** `r sum(cv$yearly_avg_Outstation_checkins == "9")`
-   **10 :** `r sum(cv$yearly_avg_Outstation_checkins == "10")`
-   **11 :** `r sum(cv$yearly_avg_Outstation_checkins == "11")`
-   **12 :** `r sum(cv$yearly_avg_Outstation_checkins == "12")`
-   **13 :** `r sum(cv$yearly_avg_Outstation_checkins == "13")`
-   **14 :** `r sum(cv$yearly_avg_Outstation_checkins == "14")`
-   **15 :** `r sum(cv$yearly_avg_Outstation_checkins == "15")`
-   **16 :** `r sum(cv$yearly_avg_Outstation_checkins == "16")`
-   **17 :** `r sum(cv$yearly_avg_Outstation_checkins == "17")`
-   **18 :** `r sum(cv$yearly_avg_Outstation_checkins == "18")`\
-   **20 :** `r sum(cv$yearly_avg_Outstation_checkins == "20")`\
-   **21 :** `r sum(cv$yearly_avg_Outstation_checkins == "21")`
-   **22 :** `r sum(cv$yearly_avg_Outstation_checkins == "22")`
-   **23 :** `r sum(cv$yearly_avg_Outstation_checkins == "23")`\
-   **24 :** `r sum(cv$yearly_avg_Outstation_checkins == "24")`
-   **25 :** `r sum(cv$yearly_avg_Outstation_checkins == "25")`
-   **26 :** `r sum(cv$yearly_avg_Outstation_checkins == "26")`\
-   **27 :** `r sum(cv$yearly_avg_Outstation_checkins == "27")`
-   **28 :** `r sum(cv$yearly_avg_Outstation_checkins == "28")`
-   **29 :** `r sum(cv$yearly_avg_Outstation_checkins == "29")`
-   **30 :** `r sum(cv$yearly_avg_Outstation_checkins == "30")`
-   **Asterisk :** `r sum(cv$yearly_avg_Outstation_checkins == "*")`
-   **Empty :** `r sum(cv$yearly_avg_Outstation_checkins == "")`

```{r}
# Assuming cv is your data frame
cv$yearly_avg_Outstation_checkins <- as.numeric(as.character(cv$yearly_avg_Outstation_checkins))


# Assuming cv is your data frame
cv$YearlyAvgCheckIns <- ifelse(cv$yearly_avg_Outstation_checkins <= 5, "1-5",
                               ifelse(cv$yearly_avg_Outstation_checkins <= 10, "5-10",
                                      ifelse(cv$yearly_avg_Outstation_checkins <= 15, "10-15",
                                             ifelse(cv$yearly_avg_Outstation_checkins <= 20, "15-20", "20+")
                                      )
                               ))

```

Impute the mode in the missing values

```{r}
# Function to impute mode
impute_mode <- function(x) {
  mode_value <- names(sort(table(x), decreasing = TRUE))[1]
  x[x == "NA" | is.na(x)] <- mode_value
  return(x)
}

# Impute mode for the 'YearlyAvgCheckIns' column
cv$YearlyAvgCheckIns <- impute_mode(cv$YearlyAvgCheckIns)
```

#### 6.2.5.1. Graphing the **YearlyAvgCheckIns** column

```{r}
cv %>%
  group_by(YearlyAvgCheckIns, Taken_product) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = reorder(factor(YearlyAvgCheckIns), count), y = count, fill = Taken_product)) +
  geom_bar(stat = "identity", position = "fill") +
  coord_flip() +
  labs(title = "Average Customer Check-In and Ticket Purchase Proportion",
       x = "Yearly Average Customer Check-In's",
       y = "Proportion",
       fill = "Buys a ticket next month") +
  scale_fill_brewer(palette = "Paired") +  # Use "Paired" palette
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### 6.2.7. Clean the **Adult_flag** column

The **Adult_flag** column should have only *Yes or No* values, but I found that it contains the following *`r unique(cv$Adult_flag)`*. Which are distributed by the following order:

-   **0 :** `r sum(cv$Adult_flag == "0")`
-   **1 :** `r sum(cv$Adult_flag == "1")`
-   **2 :** `r sum(cv$Adult_flag == "2")`
-   **3 :** `r sum(cv$Adult_flag == "3 ")`
-   **NA :** `r sum(cv$Adult_flag == "NA")`

**Action Plan:** Collapse the **Adult_flag** values to *Yes or No*, I will make all the values greater than 0 count as a *Yes* meaning that the customer is an Adult, and the rest a *No*, the missing values will be imputed with the mode of the column after I collapse them

```{r}
# Assuming cv is your data frame
cv$IsAdult <- ifelse(cv$Adult_flag == 0, "No", "Yes")
```

Impute the null values with the mode

```{r}
# Function to impute mode
impute_mode <- function(x) {
  mode_value <- names(sort(table(x), decreasing = TRUE))[1]
  x[x == "NA" | is.na(x)] <- mode_value
  return(x)
}

# Impute mode for the 'IsAdult' column
cv$IsAdult <- impute_mode(cv$IsAdult)
```

#### 6.2.7.1. Graphing the **IsAdult** column

```{r}
cv %>%
  group_by(IsAdult, Taken_product) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = reorder(factor(IsAdult), count), y = count, fill = Taken_product)) +
  geom_bar(stat = "identity", position = "fill") +
  coord_flip() +
  labs(title = "Customer Group & Buying a Ticket",
       x = "Adult Customers",
       y = "Proportion",
       fill = "Buys a ticket next month") +
  scale_fill_brewer(palette = "Paired") +  # Use "Paired" palette
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

#### 6.3.5. Impute the median and mode of the column in all columns that have missing values

RandomForests cannot handle missing values so we have to do something. Preferably, we would not delete data, instead we impute

```{r}
# Impute the median on NA values in the column Yearly_avg_view_on_travel_page 
cv$Yearly_avg_view_on_travel_page[is.na(cv$Yearly_avg_view_on_travel_page)] = median(cv$Yearly_avg_view_on_travel_page, na.rm = TRUE)

# Impute the median on NA values in the column total_likes_on_outstation_checkin_given
cv$total_likes_on_outstation_checkin_given[is.na(cv$total_likes_on_outstation_checkin_given)] = median(cv$total_likes_on_outstation_checkin_given, na.rm = TRUE)

# Impute the median on NA values in the column Yearly_avg_comment_on_travel_page
cv$Yearly_avg_comment_on_travel_page[is.na(cv$Yearly_avg_comment_on_travel_page)] = median(cv$Yearly_avg_comment_on_travel_page, na.rm = TRUE)

# Impute the median on NA values in the column Daily_Avg_mins_spend_on_traveling_page
cv$Daily_Avg_mins_spend_on_traveling_page[is.na(cv$Daily_Avg_mins_spend_on_traveling_page)] = median(cv$Daily_Avg_mins_spend_on_traveling_page, na.rm = TRUE)

# Impute the median on NA values in the column Daily_Avg_mins_spend_on_traveling_page
cv$yearly_avg_Outstation_checkins[is.na(cv$yearly_avg_Outstation_checkins)] = median(cv$yearly_avg_Outstation_checkins, na.rm = TRUE)

```

#### 6.3.5. Remove the non-wanted columns

```{r}
cv = subset(cv, select = -c(UserID, preferred_device, preferred_location_type, member_in_family, following_company_page, member_in_family, Adult_flag, working_flag))
#head(cv)
```

#### 6.3.7. Rename the column header to a more readable name

```{r}
# Dictionary mapping old column names to new names
column_name_mapping <- c(
  "Taken_product"                                 = "BuysProduct",                              
  "Yearly_avg_view_on_travel_page"                = "YearlyAvgPageVisits",           
  "total_likes_on_outstation_checkin_given"       = "TotalCheckInLikesGiven",   
  "yearly_avg_Outstation_checkins"                = "YearlyAvgCheckIns",       
  "Yearly_avg_comment_on_travel_page"             = "YearlyAvgComments",       
  "total_likes_on_outofstation_checkin_received"  = "TotalCheckInLikesReceived",
  "week_since_last_outstation_checkin"            = "WeeksSinceLastCheckIn",        
  "montly_avg_comment_on_company_page"            = "MonthAvgComments",         
  "travelling_network_rating"                     = "TravelNetworkRating",                   
  "Daily_Avg_mins_spend_on_traveling_page"        = "Avg_DailyMinPerView",
  "CompanyFollower"                               = "CompanyFollower",
  "FamilyMembers"                                 = "FamilyMembers",
  "IsWorking"                                     = "IsWorking" ,
  "PreferedDevice"                                = "PreferedDevice" ,
  "PreferredLocation"                             = "PreferredLocation",
  "YearlyAvgCheckIns"                             = "YearlyAvgCheckIns" , 
  "IsAdult"                                       = "IsAdult"
 )

# Rename the columns
names(cv) <- column_name_mapping

```

```{r}
# Save the dataframe to the same CSV file
write.csv(cv, 'Data/CustomerBehaviour.csv', row.names = FALSE)
# Saving clean dataset 
summary(cv)
```

# 7. Predictive Modelling

For my predictive modelling, I choose the Random Forest given its simplicity to execute. Some of reasons why this model was the correct for this data and research problem was:

**1. Automatic Variable Selection:** 

- We get automatic variable selection, importance ranking so I am able to simply and fast collect the best variables for later tunning.

**2. Ensemble Benefits:** 

- Random Forest is an ensemble method, combining multiple trees to improve accuracy and generalization, making it more robust than individual models.

**3. Reduction of Overfitting:** 

- The algorithm's inherent randomness helps mitigate over fitting, enhancing the model's ability to generalize well to unseen data.

**4. Handling Complexity:** 

- Random Forest excels at capturing complex relationships and non-linear patterns in data, making it suitable for classification tasks with intricate decision boundaries.

## 7.1.Clean the environmnet

```{r}
rm(list = ls())
```

## 7.2.Load packages required

```{r}
library(rpart)
library(rpart.plot)
library(ggplot2)
library(pROC)
library(explore)
library(RColorBrewer)
library(tidyverse)
library(dplyr)
library(kableExtra)
library(rmarkdown)
library(randomForest)
```

## 7.3. Read in cleaned data

```{r}
# Importing clean dataset
cb_data <- read.csv('Data/CustomerBehaviour.csv', stringsAsFactors = TRUE)

```

## 7.4. Splitting data for train and test

## 7.4.1. Set seed for reproducibility

```{r}
RNGkind(sample.kind = "default")
set.seed(2291352)
```

## 7.4.2. Split train and test data

I will be using a split of 80% for training and 20% for test data

```{r}
train.idx = sample(x=1:nrow(cb_data), size = floor(0.8*nrow(cb_data)))
train.df = cb_data[train.idx,]
test.df = cb_data[-train.idx,]
```

## 7.5. Fitting a baseline model

```{r}
myforest = randomForest(BuysProduct ~ .,
              data   = train.df,
              ntree  = 1000,
              mtry   = 4
              )
```

## 7.6. Tunning a random forest

Here I will tune a model by fitting and plotting the best (mtry), or in other words, the number of variables to randomly sample as candidates at each split. I am actually interested in finding the mtry that accounts for the lowest Out-of-Bag Error (OOB Error Rate), this rate is a measure of how well the model generalizes to unseen data.

Therefore, my tuning will be to loop trough all the possible mtry's (number of variables the dataset has), record the OOB Error Rate and then pick the lowest for my further tunned model.

```{r}
# A sequence of B (# of trees) that we want to try, we'll loop through each unique element in the vector below
mtry <- seq(1, 16)

# Make room for B, OOB error
keeps <- data.frame(mtry = rep(NA, length(mtry)),
                    OOB_err_rate = rep(NA, length(mtry)))

for (idx in 1:length(mtry)) {
  print(paste0("Fitting m = ", mtry[idx]))
  tempforest <- randomForest(BuysProduct ~ .,
                              data = train.df,
                              ntree = 1000,
                              mtry = mtry[idx])  # Dynamically changing mtry value
  
  # Record how many trees we tried
  keeps[idx, "mtry"] <- mtry[idx]
  
  # Record what our OOB error rate was
  keeps[idx, "OOB_err_rate"] <- mean(predict(tempforest) != train.df$BuysProduct)
}

# Find the row with the minimum OOB error rate
min_error_row <- keeps[which.min(keeps$OOB_err_rate), ]


p <- ggplot(data = keeps) +
  geom_line(aes(x = mtry, y = OOB_err_rate)) +
  geom_vline(xintercept = min_error_row$mtry, linetype = "dashed", color = "red") +
  geom_text(aes(label = min_error_row$mtry, x = min_error_row$mtry, y = min_error_row$OOB_err_rate + 0.01), color = "red") +
  theme_bw() +
  labs(x = "mtry", y = "OOB error rate")

print(p)

```

## 7.7. Tunning a random forest based on the lowest mtry

Now that I have tunned the models, it seems like the lowest OOB Error Rate was found at the **`r min_error_row$mtry`th**, so then I can use that to fine-tune the model.

Given that the "best mtry", might vary and for reproducibility purposes, the "best mtry" value is directly picked from the above tuning in the fine-tunned model below. By doing so, I make sure that this code is more robust and allows adaptability to changes in new data.

```{r }
FinalForest = randomForest(BuysProduct ~ .,
              data   = train.df,
              ntree  = 1000,
              mtry   =  min_error_row$mtry,
              importance = TRUE
              )
FinalForest
```

## 7.8. Declaring the value of interest

In this case, I am interested in when the customer DOES buy a ticket next month. The variable in this case that refers to that is the predicted value BuysProduct = Yes, so I specify that in the code below.

```{r}
pi_hat = predict(FinalForest, test.df, type = "prob")[,"Yes"]
```

## 7.9. Plot the ROC Curve

In order to understand the model predictive capabilities, I will plot the ROC Curve from where I expect to extract: Sensitivity, Specificity and AUC. The following are explained below:

*Sensitivity (True Positive Rate):* How good the test is at finding positive cases. *Specificity (True Negative Rate):* How good the test is at correctly identifying negative cases. *AUC:* Represents the ability of a model to distinguish between positive and negative examples.

```{r}
rocCurve = roc(response = test.df$BuysProduct,
               predictor = pi_hat,
               levels = c("No","Yes"))
plot(rocCurve,print.thres = TRUE, print.auc = TRUE)

pi_star <- coords(rocCurve, "best", ret = "threshold")$threshold[1]
sens <- coords(rocCurve, "best", ret = "sensitivity")$sensitivity
spec <- coords(rocCurve, "best", ret = "specificity")$specificity
AUC <- as.numeric(rocCurve$auc)
```

## 7.9.1. Interpreting the ROC Curve results

So, we may translate the above graph statistics to:

-  **π* ** = `r round(100*pi_star, digits = 2)`, π* or Pi Star means that we would predict that a customer will buy a ticket next month when the probability of a cusomter buying a next month is of  `r round(100*pi_star, digits = 2)`.
-  **AUC** = `r round(100*AUC, digits = 2)` (varies between 50% and 100% - closer to 1 is better).
-  **Sensitivity:** `r sens`, We correctly predict when a customer **will BUY** a ticket next month `r round(100*sens, digits = 2)`% of the time, when the customer ended up buying a ticket next month.
-  **Specificity:** `r spec`, We correctly predict when a customer **will NOT BUY** the ticket next month `r round(100*spec, digits = 2)`% of the time, when the customer never ended up buying a ticket next month.

*The Sensitivity and Specificity shown above will only be archived if we set the Pi Star to* `r round(100*pi_star, digits = 2)`%

## 7.10. Extracting the feature importance

By looking at the feature importance I expect to to understand how often and how much each feature contributes to making accurate predictions across all the trees in the forest.

```{r}
# Make a column of predicted values (Yes or No)
pi_star = coords(rocCurve, "best", ret = "threshold")$threshold
test.df$forestPred= ifelse(pi_hat>pi_star, "Yes", "No")
```

## 7.11. Plotting the feature importance

```{r}
# Extract variable importance
vi <- data.frame(Variable = rownames(FinalForest$importance),
                 Importance = FinalForest$importance[, "MeanDecreaseAccuracy"])

# Create the variable importance plot
library(ggplot2)
ggplot(data = vi) +
  geom_bar(aes(x = reorder(Variable, Importance), 
               weight = Importance),
           position = "identity") +
  coord_flip() +
  labs(x = "Variable Name", y = "Importance") +
  ggtitle("Variable Importance Plot for FinalForest Model")
```

# 8. Descriptive Modelling

In my analysis, the selection of a Binomial Generalized Linear Model (GLM) with a logit link was deliberate and guided by specific considerations:

**1. Binary Outcome Nature:** 

- The binary nature of our outcome variable ("Purchase_bin") aligns seamlessly with the inherent strengths of a Binomial GLM.

**2. Interpretability through Odds Ratios:** 

- The use of logistic regression with a logit link ensures easy interpretation, especially through odds ratios, enhancing the clarity of our findings.

**3. Systematic Feature Inclusion:** 

- A stepwise approach incorporates features based on importance, controlling model complexity and focusing on relevant variables.

**4. Incorporation of Important Features:** 

- Utilizing feature importance rankings from a Random Forest model enriches our logistic regression, capturing nuanced relationships.

**5. AIC and BIC Model Evaluation:** 

- Models are assessed using AIC and BIC, aiding in the selection of the most suitable model by balancing goodness of fit and simplicity.

**6. Adaptability to Binomial Data:** 

- GLMs are tailored for binomial outcomes, aligning the statistical framework with the distinctive characteristics of our data.

## 8.1. Create a bernoulli random variable

```{r }
cb_data$Purchase_bin = ifelse(cb_data$BuysProduct == "Yes",1,0)
```

## 8.2. Fit models with different x variables based on the feature selection results

I will extract the feature importance variables from the above plot in order of importance, so that I can later extract the features for each different model. In other words, I will run trough all the models possible based on the importance plot to predict our predicted variable. I will start with the first model only having the most important variable shown on the graph, then I will add one feature at a time, fit that and extract the AIC and BIC.

```{r}
# Order variable names by importance in descending order
variable_order <- vi$Variable[order(-vi$Importance)]

# Create an empty dataframe to store results
result_df <- data.frame(Model = character(), AIC = numeric(), BIC = numeric(), Features = character(), stringsAsFactors = FALSE)

# Loop through each model
for (i in seq_along(variable_order)) {
  # Build formula for the model
  formula <- as.formula(paste("Purchase_bin ~", paste(variable_order[1:i], collapse = " + ")))

  # Fit the model
  model <- glm(formula, data = cb_data, family = binomial(link = "logit"))

  # Get AIC and BIC
  aic_value <- AIC(model)
  bic_value <- BIC(model)

  # Get model features
  features <- paste(variable_order[1:i], collapse = ", ")

  # Store results in the dataframe
  result_df <- rbind(result_df, data.frame(Model = paste("model_", i, sep = ""), AIC = round(aic_value), BIC = round(bic_value), Features = features, stringsAsFactors = FALSE))
}

```

## 8.3. Compare the results of each model

The follwoing graph will showcase the AIC and BIC for each model made, here are the variables that each model had

-   **Model 1 :** `r result_df$Features[1]`
-   **Model 2 :** `r result_df$Features[2]`
-   **Model 3 :** `r result_df$Features[3]`
-   **Model 4 :** `r result_df$Features[4]`
-   **Model 5 :** `r result_df$Features[5]`
-   **Model 6 :** `r result_df$Features[6]`
-   **Model 7 :** `r result_df$Features[7]`
-   **Model 8 :** `r result_df$Features[8]`
-   **Model 9 :** `r result_df$Features[9]`
-   **Model 10 :** `r result_df$Features[10]`
-   **Model 11 :** `r result_df$Features[11]`
-   **Model 12 :** `r result_df$Features[12]`
-   **Model 13 :** `r result_df$Features[13]`
-   **Model 14 :** `r result_df$Features[14]`
-   **Model 15 :** `r result_df$Features[15]`
-   **Model 16 :** `r result_df$Features[16]`

```{r }
result_df$Model <- factor(result_df$Model, levels = unique(result_df$Model))

# ggplot code
ggplot(data=result_df, aes(x=Model, y=AIC, group=1)) +
  geom_line(aes(color = "AIC")) +
  geom_point(aes(color = "AIC")) +
  
  geom_line(aes(y = BIC, color = "BIC")) +
  geom_point(aes(y = BIC, color = "BIC")) +
  
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  
  # Customize colors
  scale_color_manual(values = c("AIC" = "blue", "BIC" = "red"))+
  
  # Label y-axis
  labs(y = "AIC & BIC")

```

Although I am plotting both AIC and BIC, I will use the BIC statistic given the the size of our data could be considered big (number of rows in dataset: **`r nrow(cb_data)`**) and usually for cases where n is larger, more parsimonious criteria, such as BIC, often seem better. On the other hand, cases where n is small, criteria with lower under-fitting rates, AIC could work better.

## 8.4. Final Model Variable Selection

That being said, looking at the above graph I could have chosen Model 15 given that it provides the lowest **BIC**: `r result_df$BIC[15]` but given the purpose of this model (Descriptive), I am taking into account that a more modest model with less features to interpret will help with the interpretation of the model and the feature importance.

Additionally, looking back at the variable importance plot, the predictor variables that are added to the models after Model 7 (Model 8, Model 9..) represent a 37% or lower model importance, so that provides even more reasons to use Model 7 as my predictor model of choice.

### 8.4.1. Model with lowest BIC

**Model 15 Predictor Variables:** `r result_df$Features[15]`

### 8.4.2. Modest model

**Model 7 Predictor Variables:** `r result_df$Features[7]`

## 8.5. Fitting chosen model

```{r}
FinalModel = glm(BuysProduct ~ TotalCheckInLikesReceived + CompanyFollower + YearlyAvgPageVisits 
                 + IsAdult + TotalCheckInLikesGiven + WeeksSinceLastCheckIn + YearlyAvgCheckIns, 
          data = cb_data, 
          family = binomial(link = "logit"))

FinalModel
```

```{r}

beta <- coef(FinalModel)
beta

exp(beta)

confint(FinalModel)
```

## 8.6. Extracting Model Coefficients

```{r}
# Extracting coefficient estimates
betahat <- coef(FinalModel)

# Extracting the diagonal of the covariance matrix
var_betahat <- vcov(FinalModel) %>% diag

# Calculating the quantile for a 95% confidence interval
quantile <- qnorm(c(0.025, 0.975))

# Calculating lower and upper bounds
lower <- betahat - sqrt(var_betahat) * quantile[2]
upper <- betahat + sqrt(var_betahat) * quantile[2]

# Exponentiating to get odds ratio CI's
odds_ratios <- exp(betahat)
lower_odds <- exp(lower)
upper_odds <- exp(upper)

# Creating a data frame with variable names and results
conf_df <- data.frame(
  Variable = names(betahat),
  Odds_Ratio = odds_ratios,
  Lower_Bound = lower_odds,
  Upper_Bound = upper_odds)

# Adding the OddsResult column based on the condition
conf_df$OddsResult <- ifelse(conf_df$Odds_Ratio >= 1, "Increase", "Decrease")

# Calculating the factor as a percentage and formatting it
conf_df$Factor_Percentage <- sprintf("%.4f%%", abs(1 - conf_df$Odds_Ratio) * 100)

# Printing the results
conf_df

```

## 8.7. Model Coefficient Interpretation

**TotalCheckInLikesReceived:** For each additional like the customer has given in other check-ins, the odds of a customer buying a ticket next month are estimated to **`r conf_df$OddsResult[2]`** by a factor of approximately `r conf_df$Odds_Ratio[2]`.

-   This means that, on average, the odds of the event (customer buying a ticket) occurring `r conf_df$OddsResult[2]` by approximately `r conf_df$Factor_Percentage[2]` for each additional like in other check-ins, holding other variables constant. The associated 95% confidence interval for this odds ratio is between approximately `r conf_df$Lower_Bound[2]` and `r conf_df$Upper_Bound[2]`.

**CompanyFollowerYes:** If the individual follows the company account (CompanyFollowerYes),the odds of a customer buying a ticket next month are estimated to **`r conf_df$OddsResult[3]`** by a factor of approximately `r conf_df$Odds_Ratio[3]`, times the odds of compared to a customer that does not follow the company account, holding all other vaiables constant.

-   This means that, on average, the odds of the event (customer buying a ticket) occurring `r conf_df$OddsResult[3]` by approximately `r conf_df$Factor_Percentage[3]`for each additional like in other check-ins, holding other variables constant. The associated 95% confidence interval for this odds ratio is between approximately `r conf_df$Lower_Bound[3]` and `r conf_df$Upper_Bound[3]`.

**YearlyAvgPageVisits:** For each additional customer visit to the website on average, the odds of a customer buying a ticket next month are estimated to **`r conf_df$OddsResult[4]`** by a factor of approximately `r conf_df$Odds_Ratio[4]`.

-   This means that, on average, the odds of the event (customer buying a ticket) occurring `r conf_df$OddsResult[4]` by approximately `r conf_df$Factor_Percentage[4]` for each visit to the website on average per year, holding other variables constant. The associated 95% confidence interval for this odds ratio is between approximately `r conf_df$Lower_Bound[4]` and `r conf_df$Upper_Bound[4]`.

## 9. Graphing Model Variables

After the model coefficient interpretations, I will include some graphs to better understand the interpretations.

### 9.1. Graphing Total Likes and Product Purchase

Understanding how the customer being an adult or not and the likes they have given to previous destinations where they have Checked-In in the past affects whether they will buy a ticket next month or not.

```{r}
ggplot(data = cb_data)+
  geom_histogram(aes(x = TotalCheckInLikesReceived, fill = BuysProduct), position = "fill", binwidth = 800) + 
  scale_fill_brewer("Customer\nBuys Product", palette = "Paired")+
  labs(x = "Total Check In Likes", y = "Proportion") +
  facet_wrap(~IsAdult, labeller = labeller(IsAdult = c(No = "Not Adult", Yes = "Adult")))
```

### 9.2. Graphing Yearly Average Page Visits and Product Purchase

Understanding how the customer being a follower or not and the yearly average number of visits to a certain page will affect to whether they will buy a ticket next month or not.

```{r}
ggplot(data = cb_data)+
  geom_histogram(aes(x = YearlyAvgPageVisits, fill = BuysProduct), position = "fill", binwidth = 30) + 
  scale_fill_brewer("Customer\nBuys Product", palette = "Paired")+
  labs(x = "Yearly Avg Page Visits", y = "Proportion") +
  facet_wrap(~CompanyFollower, labeller = labeller(CompanyFollower = c(No = "Not Follower", Yes = "Follows")))
```

### 9.3. Graphing Yearly Average Check-In's and Product Purchase

Understanding how the customer being an adult or not and the yearly number of check-in's they have completed in the past affects whether they will buy a ticket next month or not.

```{r}
ggplot(data = cb_data)+
  geom_histogram(aes(x = YearlyAvgCheckIns, fill = BuysProduct), position = "fill", binwidth = 2) + 
  scale_fill_brewer("Customer\nBuys Product", palette = "Paired")+
  labs(x = "Yearly Avg Cutomer Check In's", y = "Proportion") +
  facet_wrap(~IsAdult, labeller = labeller(IsAdult = c(No = "Not Adult", Yes = "Adult")))
```

### 9.4. Graphing Yearly Average Check-In's and Product Purchase

Understanding how the customer being an adult or not and the average number of pages they visited might affect whether the customer will buy a ticket next month or not.

```{r}
ggplot(data = cb_data)+
  geom_histogram(aes(x = YearlyAvgPageVisits, fill = BuysProduct), position = "fill", binwidth = 30) + 
  scale_fill_brewer("Customer\nBuys Product", palette = "Paired")+
  labs(x = "Yearly Avg Cutomer Page Visits", y = "Proportion") +
  facet_wrap(~IsAdult, labeller = labeller(IsAdult = c(No = "Not Adult", Yes = "Adult")))
```

### 9.5. Graphing Total Check-In's Received and Product Purchase

Understanding how the total check-in likes received and the average number of pages a customer visits might affect whether the customer might buy a ticket next month or not.

```{r}
ggplot(data = cb_data) +
  geom_point(aes(x =TotalCheckInLikesReceived, y = YearlyAvgPageVisits , colour = BuysProduct)) +
  labs(x = "Total Check In Likes Received", y = "Yearly Avg Page Visits") +
  scale_colour_brewer("Customer\nBuys Product", palette = "Paired")
```

## 10. Recommendations

### 10.1. Strategic Social Media Engagement:

Let's develop a targeted social media engagement strategy to encourage more customers to follow our company's account. Given the substantial positive impact of being a CompanyFollower, increasing our follower base has the potential to significantly boost ticket sales.

### 10.2. Enhanced Online Experience:

Despite the small impact, let's consider optimizing our website to enhance the user experience. This could involve personalized content recommendations, targeted promotions, or other features aimed at increasing the conversion rate for website visits to ticket purchases.

### 10.3. Continuous Monitoring and Iteration:

We'll establish a routine for monitoring the model's performance and iterate as needed. While Model 7 has been chosen for its balance between simplicity and performance, periodic reassessment should be conducted to ensure continued relevance and accuracy.

## 11. Conclusions

### 11.1. Model Validation for Confidence:

Rest assured that the selected Model 7 demonstrates robust predictive performance, with a high AUC of 99.88%. The model's ability to correctly identify both positive and negative cases (sensitivity and specificity exceeding 99%) instills confidence in its reliability.

### 11.2. Key Drivers of Ticket Purchases:

CompanyFollower emerges as a key driver of ticket purchases, emphasizing the importance of building and maintaining a strong social media presence. While social interactions (TotalCheckInLikesReceived) play a role, their impact is relatively small and may require a more nuanced understanding.

### 11.3. Balancing Complexity and Efficiency:

I've opted for Model 7 over more complex models based on the variable importance plot, with variables beyond Model 7 contributing 37% or less to the model. This choice strikes a balance between model performance and computational efficiency, ensuring we get the most value with the least complexity.

### 11.4. Decision-Making Threshold:

The suggested probability threshold of 0.2325 provides a balanced trade-off between sensitivity and specificity. This threshold can be fine-tuned based on our business considerations and the acceptable level of false positives and false negatives.

### 11.5. Continuous Improvement Journey:

Let's emphasize the importance of ongoing monitoring and adaptation. As our customers' behaviors and preferences evolve, the model should be regularly updated to ensure its continued relevance and accuracy in predicting ticket purchases.

# 12. References

1.  [R Markdowon](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)
2.  [Dataset](https://www.kaggle.com/datasets/ddosad/customer-behaviour-tourism-portal)
